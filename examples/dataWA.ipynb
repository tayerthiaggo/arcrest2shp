{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "# import json\n",
    "\n",
    "import subprocess\n",
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, urljoin\n",
    "import re\n",
    "\n",
    "import folium\n",
    "import csv\n",
    "import concurrent.futures\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dwae_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder (out_path, folder_name='extracted_data'):\n",
    "    \"\"\"\n",
    "    Create a folder in the specified output path.\n",
    "    Args:\n",
    "        out_path (str): Output path where the folder will be created.\n",
    "        folder_name (str): Name of the folder to be created.\n",
    "\n",
    "    Returns:\n",
    "        str: Export path of the created folder.\n",
    "\n",
    "    Raises:\n",
    "        OSError: If there is an error creating the folder.\n",
    "\n",
    "    \"\"\"\n",
    "    export_path = os.path.join(out_path, folder_name)\n",
    "    try:\n",
    "        # Create the folder\n",
    "        os.makedirs(export_path)\n",
    "        print(f\"Folder created: {export_path}\")\n",
    "    except:\n",
    "        print('Folder already exists', export_path)\n",
    "\n",
    "    return export_path\n",
    "\n",
    "\n",
    "def bbox_shp (shp, crs=7844):\n",
    "    \"\"\"\n",
    "    Get the bounding box coordinates of a shapefile.\n",
    "\n",
    "    Args:\n",
    "        shp (str): Path to the shapefile.\n",
    "        crs (int, optional): Coordinate reference system (CRS) code. Default is 7844.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the bounding box coordinates (list) and the CRS code (int).\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read the shapefile using geopandas\n",
    "    gdf = gpd.read_file(shp)\n",
    "\n",
    "    # Convert the geometry to the specified CRS and calculate the bounding box\n",
    "    bbox = list(gdf.to_crs(crs).total_bounds)\n",
    "\n",
    "    return bbox, crs\n",
    "    # return json.dumps(geometry_dict), crs\n",
    "\n",
    "    \n",
    "def run_esri2geojson (url, bbox, crs, layer_name, export_path):\n",
    "    \"\"\"\n",
    "    Runs the 'esri2geojson' command-line tool to convert data from an Esri service to GeoJSON format.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the Esri service.\n",
    "        bbox (list): A list of four float values representing the bounding box coordinates in the order [minx, miny, maxx, maxy].\n",
    "        crs (str): The coordinate reference system (CRS) identifier.\n",
    "        gjson_out_path (str): The file path for the output GeoJSON file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Raises:\n",
    "        subprocess.CalledProcessError: If the 'esri2geojson' command fails to execute.\n",
    "\n",
    "    Example:\n",
    "        url = 'https://services.slip.wa.gov.au/public/rest/services/SLIP_Public_Services/Boundaries/MapServer/2'\n",
    "        bbox = [115.8444528, -31.98380876, 116.15245686, -31.70508152]\n",
    "        crs = '4326'\n",
    "        gjson_out_path = 'E:\\Scripts\\idot_roads5.geojson'\n",
    "        run_esri2geojson(url, bbox, crs, gjson_out_path)\n",
    "    \"\"\"\n",
    "    \n",
    "    geojson_out_path = os.path.join(export_path, 'geojson', f'{layer_name}.geojson')\n",
    "\n",
    "    # Edit geometry for input\n",
    "    geometry_str = ''.join(['geometry=', ','.join([str(num) for num in bbox])])\n",
    "\n",
    "    # Edit crs for input \n",
    "    crs_str = ''.join(['inSR=', str(crs)]) \n",
    "\n",
    "    # Concatenate the variables with spaces\n",
    "    command = ' '.join(['esri2geojson',url, '-p', geometry_str, '-p', crs_str, geojson_out_path])\n",
    "\n",
    "    # Execute the command\n",
    "    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
    "\n",
    "    # Check the result\n",
    "    if result.returncode == 0:\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "        print(f'{url} An error occurred while executing the run_esri2geojson.')\n",
    "\n",
    "    return geojson_out_path\n",
    "\n",
    "\n",
    "def clip_geojson_export_shp (shp, crs,  geojson_out_path, shp_out_path):\n",
    "    \"\"\"\n",
    "    Clips a GeoJSON file using a polygon GeoDataFrame and exports the clipped data to a shapefile.\n",
    "\n",
    "    Args:\n",
    "        aoi_gdf (GeoDataFrame): A GeoDataFrame representing the area of interest polygon for clipping.\n",
    "        gjson_out_path (str): The file path of the input GeoJSON file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Example:\n",
    "        import geopandas as gpd\n",
    "\n",
    "        # Define the area of interest as a polygon GeoDataFrame\n",
    "        aoi_polygon = gpd.read_file('path/to/aoi.shp')\n",
    "\n",
    "        # Specify the input GeoJSON file\n",
    "        input_gjson = 'path/to/input.geojson'\n",
    "\n",
    "        # Call the function to clip and export\n",
    "        clip_geojson_export_shp(aoi_polygon, input_gjson)\n",
    "    \"\"\"\n",
    "\n",
    "    # Read the GeoJSON and shapefile into GeoDataFrames\n",
    "    geojson_gdf = gpd.read_file(geojson_out_path).to_crs(crs)\n",
    "    \n",
    "    if geojson_gdf.empty:\n",
    "        pass\n",
    "    else:\n",
    "        shp_gdf = gpd.read_file(shp).to_crs(crs)\n",
    "\n",
    "        # Clip the GeoJSON with the shapefile\n",
    "        clipped_gdf = gpd.clip(geojson_gdf, shp_gdf)\n",
    "\n",
    "        # Shorten the column names to a maximum of 10 characters\n",
    "        clipped_gdf = clipped_gdf.rename(columns=lambda x: x[:10])\n",
    "\n",
    "        # Extract the file name without extension\n",
    "        output_shapefile = f'{os.path.splitext(os.path.basename(geojson_out_path))[0]}.shp'\n",
    "\n",
    "        # Export the clipped GeoDataFrame to a shapefile\n",
    "        clipped_gdf.to_file(os.path.join(shp_out_path, output_shapefile), driver='ESRI Shapefile')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_links (url, cache):\n",
    "    \"\"\"\n",
    "    Retrieve all links from a webpage.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the webpage.\n",
    "        cache (dict): A dictionary to cache previously retrieved links.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of links found on the webpage.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if url in cache:\n",
    "        return cache[url]\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    # Check if the request was successful (HTTP status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Find all <li> elements\n",
    "        li_tags = soup.find_all('li')\n",
    "\n",
    "        links = []\n",
    "\n",
    "        for li in li_tags: \n",
    "                    \n",
    "            if li.find_all('a'):\n",
    "                # has_links = True\n",
    "                # Find all <a> elements within the <li>\n",
    "                a_tags = li.find_all('a')\n",
    "                for a in a_tags:            \n",
    "                    # Extract the link URL from the <a> element\n",
    "                    link = urljoin(url, a.get('href'))\n",
    "                    links.append(link)    \n",
    "        return links\n",
    "    else:\n",
    "        print(url, \"\\nRequest failed with status code:\", response.status_code)\n",
    "        \n",
    "        return []\n",
    "\n",
    "def process_links(url, cache, visited):\n",
    "    \"\"\"\n",
    "    Recursively process links from a starting URL.\n",
    "\n",
    "    Args:\n",
    "        url (str): The starting URL to process.\n",
    "        cache (dict): A dictionary to cache previously retrieved links.\n",
    "        visited (set): A set to keep track of visited URLs.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of processed links.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if url in visited:\n",
    "        return []\n",
    "\n",
    "    visited.add(url)  # Mark the current URL as visited\n",
    "\n",
    "    links = retrieve_links(url, cache)\n",
    "    processed_links = []\n",
    "   \n",
    "    for link in links:\n",
    "        if link not in visited:\n",
    "            # Process the link recursively\n",
    "            processed_links.append(link)\n",
    "            processed_links.extend(process_links(link, cache, visited))\n",
    "\n",
    "    return processed_links\n",
    "\n",
    "def check_if_vector (soup):\n",
    "    \"\"\"\n",
    "    Check if the HTML page contains information about a vector geometry type.\n",
    "\n",
    "    Args:\n",
    "        soup (BeautifulSoup): The BeautifulSoup object representing the parsed HTML.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the page contains vector geometry type information, False otherwise.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Find all tags that contain the text \"esriGeometry\"\n",
    "    cells = soup.find_all(lambda tag: tag.name == 'b' and 'Geometry Type:' in tag.text)\n",
    "\n",
    "    # Check if any cell contains the desired text\n",
    "    contains_esriGeometry = any('esriGeometry' in cell.next_sibling.strip() for cell in cells)\n",
    "\n",
    "    if contains_esriGeometry:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_csv (file_path, columns, data):\n",
    "    # Check if the file exists\n",
    "    file_exists = os.path.isfile(file_path)\n",
    "\n",
    "    # Write the extracted information to the CSV file\n",
    "    with open(file_path, 'a', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "\n",
    "        # Write header row if the file is newly created\n",
    "        if not file_exists:\n",
    "            writer.writerow(columns)\n",
    "        # Write the new row of data\n",
    "        writer.writerow(data)\n",
    "\n",
    "def info_to_sheets (export_path, soup, layer_name, url, csv_name):\n",
    "    \"\"\"\n",
    "    Write extracted information to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        out_path (str): The output directory path.\n",
    "        soup (BeautifulSoup): The BeautifulSoup object representing the parsed HTML.\n",
    "        layer_name (str): The name of the layer.\n",
    "        url (str): The URL of the layer.\n",
    "\n",
    "    \"\"\"\n",
    "    # Extract the geometry type from the soup object\n",
    "    geometry_type = soup.find('b', string='Geometry Type:').next_sibling.strip()\n",
    "\n",
    "    # Extract the description text from the soup object\n",
    "    description_text = soup.find('b', string='Description: ').next_sibling.strip()\n",
    "\n",
    "    source = '_'.join(layer_name.split('_')[0:1])\n",
    "\n",
    "    # Get the current date\n",
    "    extraction_date = datetime.date.today()\n",
    "\n",
    "    # Define the file path\n",
    "    file_path = os.path.join(export_path, f'{csv_name}.csv')\n",
    "\n",
    "    columns = ['Source', 'Name', 'Geometry Type', 'Description', 'URL', 'Extraction Date']\n",
    "    data = [source, layer_name, geometry_type, description_text, url, extraction_date]\n",
    "\n",
    "    write_csv (file_path, columns, data)\n",
    "\n",
    "\n",
    "def filter_layer_name (soup):\n",
    "    \"\"\"\n",
    "    Filter and format the layer name extracted from the HTML soup.\n",
    "\n",
    "    Args:\n",
    "        soup (BeautifulSoup): The BeautifulSoup object representing the parsed HTML.\n",
    "\n",
    "    Returns:\n",
    "        str: The filtered and formatted layer name.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve the <h2> name ('Layer')\n",
    "    h2_tag = soup.find('h2')\n",
    "    layer_name = h2_tag.text.split(':')[1].split('(ID')[0].replace(')', '').strip()\n",
    "\n",
    "    # Check if the page has a 'Parent Layer' section\n",
    "    parent_layer_tag = soup.find('b', string='Parent Layer:')\n",
    "    if parent_layer_tag:\n",
    "        layer_name = h2_tag.text.split(':')[1].split('(')[0].strip()\n",
    "        parent_layer_link = parent_layer_tag.find_next_sibling('a')\n",
    "        if parent_layer_link:\n",
    "            parent_layer_name = parent_layer_link.text.split('(')[1].split(')')[0].strip()\n",
    "            layer_name = parent_layer_name + ' ' + layer_name\n",
    "\n",
    "    # Replace non-alphanumeric characters with underscores\n",
    "    layer_name = re.sub(r'\\W+', '_', layer_name)\n",
    "\n",
    "    layer_name = f\"{'_'.join(layer_name.split('_')[-2:])}_{'_'.join(layer_name.split('_')[:-2])}\"\n",
    "\n",
    "    return layer_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data (url, shp, export_path):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful (HTTP status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Find all <li> elements\n",
    "        for li in soup.find_all('li'):\n",
    "\n",
    "            if li.find_all('a'):\n",
    "                pass\n",
    "\n",
    "            elif check_if_vector (soup):\n",
    "\n",
    "                layer_name = filter_layer_name (soup)\n",
    "\n",
    "                csv_name = 'extracted_data'\n",
    "                    \n",
    "                info_to_sheets (export_path, soup, layer_name, url, csv_name)\n",
    "\n",
    "                bbox, crs = bbox_shp (shp)\n",
    "\n",
    "                geojson_out_path = run_esri2geojson (url, bbox, crs, layer_name, export_path)\n",
    "\n",
    "                break\n",
    "\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        return geojson_out_path\n",
    "    else:\n",
    "        print(url, \"\\nRequest failed with status code:\", response.status_code)\n",
    "\n",
    "\n",
    "# Create a function to process each URL\n",
    "def process_url(url):\n",
    "    geojson_out_path = download_data(url, shp, export_path)\n",
    "    clip_geojson_export_shp(shp, crs, geojson_out_path, shp_out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAIN\n",
    "\n",
    "# Path to the shapefile (.shp) file\n",
    "shp = r'E:\\Scripts\\SHP\\AOI_sample.shp'\n",
    "out_path = r'E:\\Scripts'\n",
    "# url_base = 'https://services.slip.wa.gov.au/public/rest/services'\n",
    "\n",
    "url_base = 'https://services.slip.wa.gov.au/public/rest/services/SLIP_Public_Services/Education/MapServer'\n",
    "crs=7844\n",
    "\n",
    "## transfer files to python file\n",
    "# import utils\n",
    "\n",
    "#create folders to export\n",
    "\n",
    "##main folder\n",
    "export_path = create_folder (out_path)\n",
    "##geojson\n",
    "geojson_out_path = create_folder (export_path, 'geojson')\n",
    "##shp\n",
    "shp_out_path = create_folder (export_path, 'shp')\n",
    "\n",
    "#retrieve all links\n",
    "cache = {}  # Dictionary to cache retrieved links\n",
    "visited = set()  # Set to track visited URLs\n",
    "\n",
    "all_links = process_links (url_base, cache, visited)\n",
    "filtered_data = [value for value in all_links if 'FS/MapServer' not in value]\n",
    "\n",
    "#retrieve bbox\n",
    "bbox, crs = bbox_shp (shp, crs)\n",
    "\n",
    "# #download data\n",
    "# Set the number of threads you want to use\n",
    "num_threads = 10  # Choose the desired number of threads\n",
    "\n",
    "# Use ThreadPoolExecutor to execute the function concurrently\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "    executor.map(process_url, filtered_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = r'E:\\Scripts\\extracted_data\\extracted_data.csv'\n",
    "\n",
    "def remove_duplicate_headings(csv_file):\n",
    "    # Read the CSV file and find duplicate headings\n",
    "    with open(csv_file, \"r\") as file:\n",
    "        reader = csv.reader(file)\n",
    "        rows = list(reader)\n",
    "\n",
    "    # Remove duplicate headings from subsequent rows\n",
    "    for i in range(1, len(rows)):\n",
    "        rows[i] = [value for value in rows[i] if value not in headings]\n",
    "\n",
    "    # Remove empty rows\n",
    "    rows = [row for row in rows if any(row)]\n",
    "\n",
    "    # Rewrite the file with modified rows\n",
    "    with open(csv_file, \"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_temp = clipped\n",
    "\n",
    "\n",
    "# Create a Folium map centered on a specific location\n",
    "map_center = [np.mean([gdf_temp.total_bounds[1],gdf_temp.total_bounds[3]]), \n",
    "              np.mean([gdf_temp.total_bounds[0],gdf_temp.total_bounds[2]])]\n",
    "\n",
    "m = folium.Map(location=map_center, zoom_start=10)\n",
    "\n",
    "# Convert the GeoPandas data to GeoJSON format\n",
    "geojson_data = gdf_temp.to_crs(epsg='4326').to_json()\n",
    "\n",
    "# Add the GeoJSON data as a GeoJSON layer to the Folium map\n",
    "folium.GeoJson(geojson_data).add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, urljoin\n",
    "import re\n",
    "\n",
    "# # URL to retrieve data from\n",
    "# url = \"https://services.slip.wa.gov.au/public/rest/services/SLIP_Public_Services/Boundaries/MapServer\"\n",
    "\n",
    "# url = \"https://services.slip.wa.gov.au/public/rest/services/SLIP_Public_Services\"\n",
    "\n",
    "\n",
    "url = 'https://services.slip.wa.gov.au/public/rest/services'\n",
    "\n",
    "# url = 'https://services.slip.wa.gov.au/public/rest/services/Geocoder'\n",
    "\n",
    "\n",
    "def retrieve_links (url):\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    # Check if the request was successful (HTTP status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Find all <li> elements\n",
    "        li_tags = soup.find_all('li')\n",
    "\n",
    "        links = []\n",
    "\n",
    "        for li in li_tags: \n",
    "\n",
    "            print(li)\n",
    "                   \n",
    "            if li.find_all('a'):\n",
    "                has_links = True\n",
    "                # Find all <a> elements within the <li>\n",
    "                a_tags = li.find_all('a')\n",
    "                for a in a_tags:            \n",
    "                    # Extract the link URL from the <a> element\n",
    "                    link = urljoin(url, a.get('href'))\n",
    "                    links.append(link)    \n",
    "\n",
    "            elif check_if_vector (url) == True and url not in links:\n",
    "                \n",
    "                try:\n",
    "                    print('EXECUTE', url)\n",
    "                        \n",
    "                    # Retrieve the <h2> name ('Layer')\n",
    "                    h2_tag = soup.find('h2')\n",
    "                    layer_name = h2_tag.text.split(':')[1].split('(ID')[0].replace(')', '').strip()\n",
    "\n",
    "                    # Check if the page has a 'Parent Layer' section\n",
    "                    parent_layer_tag = soup.find('b', string='Parent Layer:')\n",
    "                    if parent_layer_tag:\n",
    "                        layer_name = h2_tag.text.split(':')[1].split('(')[0].strip()\n",
    "                        parent_layer_link = parent_layer_tag.find_next_sibling('a')\n",
    "                        if parent_layer_link:\n",
    "                            parent_layer_name = parent_layer_link.text.split('(')[1].split(')')[0].strip()\n",
    "                            print(parent_layer_name)\n",
    "                            layer_name = layer_name + ' ' + parent_layer_name\n",
    "\n",
    "                    # Replace non-alphanumeric characters with underscores\n",
    "                    layer_name = re.sub(r'\\W+', '_', layer_name)\n",
    "    \n",
    "\n",
    "                    print(layer_name)\n",
    "\n",
    "                    break\n",
    "\n",
    "                except:\n",
    "                    print('except')\n",
    "                    pass\n",
    "\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        return links\n",
    "    else:\n",
    "        print(url, \"\\nRequest failed with status code:\", response.status_code)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# from dwae_utils import dwae\n",
    "\n",
    "from dwea import dwae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the shapefile (.shp) file\n",
    "shp = r'E:\\Scripts\\SHP\\AOI_sample.shp'\n",
    "out_path = r'E:\\Scripts\\canning'\n",
    "# url_base = 'https://services.slip.wa.gov.au/public/rest/services'\n",
    "\n",
    "url_base = 'https://services.slip.wa.gov.au/public/rest/services/SLIP_Public_Services/Education/MapServer'\n",
    "crs=7844\n",
    "num_threads = 10\n",
    "\n",
    "\n",
    "dwae (url_base, shp, out_path, crs, num_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "# from dwea import dwae\n",
    "\n",
    "# # Path to the shapefile (.shp) file\n",
    "\n",
    "# shp = r'E:\\Scripts\\SHP\\AOI_sample.shp'\n",
    "# # shp = r'E:\\Scripts\\canning\\SHP\\AOI.shp'\n",
    "# out_path = r'E:\\canning_river'\n",
    "# # url_base = 'https://services.slip.wa.gov.au/public/rest/services'\n",
    "\n",
    "\n",
    "# url_base = 'https://services.slip.wa.gov.au/public/rest/services/Landgate_Public_Maps/Map_of_Bush_Fire_Prone_Areas_3/MapServer/1'\n",
    "\n",
    "# crs=7844\n",
    "# num_threads = 10\n",
    "\n",
    "\n",
    "# dwae (url_base, shp, out_path, crs, num_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "\n",
    "def retrieve_links (url, cache):\n",
    "    \"\"\"\n",
    "    Retrieve all links from a webpage.\n",
    "    Args:\n",
    "        url (str): The URL of the webpage.\n",
    "        cache (dict): A dictionary to cache previously retrieved links.\n",
    "    Returns:\n",
    "        list: A list of links found on the webpage.\n",
    "    \"\"\"\n",
    "\n",
    "    if url in cache:\n",
    "        return cache[url]\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    # Check if the request was successful (HTTP status code 200)\n",
    "    if response.status_code != 200:\n",
    "        print(url, \"\\nRequest failed with status code:\", response.status_code)\n",
    "        return []\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all <li> elements\n",
    "    li_tags = soup.find_all('li')\n",
    "\n",
    "    links = []\n",
    "    for li in li_tags:            \n",
    "        if li.find_all('a'):\n",
    "            # Find all <a> elements within the <li>\n",
    "            a_tags = li.find_all('a')\n",
    "            for a in a_tags:            \n",
    "                # Extract the link URL from the <a> element\n",
    "                link = urljoin(url, a.get('href'))\n",
    "                links.append(link)    \n",
    "    return links\n",
    "\n",
    "def process_links(url, cache, visited):\n",
    "    \"\"\"\n",
    "    Recursively process links from a starting URL.\n",
    "\n",
    "    Args:\n",
    "        url (str): The starting URL to process.\n",
    "        cache (dict): A dictionary to cache previously retrieved links.\n",
    "        visited (set): A set to keep track of visited URLs.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of processed links.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if url in visited:\n",
    "        return []\n",
    "    visited.add(url)  # Mark the current URL as visited\n",
    "    links = retrieve_links(url, cache)\n",
    "    processed_links = []\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        for link in links:\n",
    "            if link not in visited:\n",
    "                # Process the link asynchronously\n",
    "                future = executor.submit(process_links, link, cache, visited)\n",
    "                futures.append(future)\n",
    "\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            processed_links.extend(future.result())\n",
    "\n",
    "    return processed_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_base = 'https://services.slip.wa.gov.au/public/rest/services/SLIP_Public_Services/Boundaries/MapServer'\n",
    "\n",
    "#retrieve all links\n",
    "cache = {}  # Dictionary to cache retrieved links\n",
    "visited = set()  # Set to track visited URLs\n",
    "\n",
    "all_links = process_links (url_base, cache, visited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_links (url, cache):\n",
    "    \"\"\"\n",
    "    Retrieve all links from a webpage.\n",
    "    Args:\n",
    "        url (str): The URL of the webpage.\n",
    "        cache (dict): A dictionary to cache previously retrieved links.\n",
    "    Returns:\n",
    "        list: A list of links found on the webpage.\n",
    "    \"\"\"\n",
    "\n",
    "    if url in cache:\n",
    "        return cache[url]\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    # Check if the request was successful (HTTP status code 200)\n",
    "    if response.status_code != 200:\n",
    "        print(url, \"\\nRequest failed with status code:\", response.status_code)\n",
    "        return []\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all <li> elements\n",
    "    li_tags = soup.find_all('li')\n",
    "\n",
    "    links = []\n",
    "    for li in li_tags:            \n",
    "        if li.find_all('a'):\n",
    "            # Find all <a> elements within the <li>\n",
    "            a_tags = li.find_all('a')\n",
    "            for a in a_tags:            \n",
    "                # Extract the link URL from the <a> element\n",
    "                link = urljoin(url, a.get('href'))\n",
    "                links.append(link)    \n",
    "    return links\n",
    "\n",
    "\n",
    "def process_links(url, cache, visited):\n",
    "    \"\"\"\n",
    "    Recursively process links from a starting URL.\n",
    "\n",
    "    Args:\n",
    "        url (str): The starting URL to process.\n",
    "        cache (dict): A dictionary to cache previously retrieved links.\n",
    "        visited (set): A set to keep track of visited URLs.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of processed links.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if url in visited:\n",
    "        return []\n",
    "    visited.add(url)  # Mark the current URL as visited\n",
    "    links = retrieve_links(url, cache)\n",
    "    processed_links = []\n",
    "  \n",
    "    for link in links:\n",
    "        if link not in visited:\n",
    "            # Process the link recursively\n",
    "            processed_links.append(link)\n",
    "            processed_links.extend(process_links(link, cache, visited))\n",
    "    return processed_links\n",
    "\n",
    "\n",
    "\n",
    "# def process_links(url, cache, visited):\n",
    "#     \"\"\"\n",
    "#     Recursively process links from a starting URL.\n",
    "\n",
    "#     Args:\n",
    "#         url (str): The starting URL to process.\n",
    "#         cache (dict): A dictionary to cache previously retrieved links.\n",
    "#         visited (set): A set to keep track of visited URLs.\n",
    "\n",
    "#     Returns:\n",
    "#         list: A list of processed links.\n",
    "\n",
    "#     \"\"\"\n",
    "\n",
    "#     if url in visited:\n",
    "#         return []\n",
    "#     visited.add(url)  # Mark the current URL as visited\n",
    "#     links = retrieve_links(url, cache)\n",
    "#     processed_links = []\n",
    "  \n",
    "#     for link in links:\n",
    "#         if link not in visited:\n",
    "#             # Process the link recursively\n",
    "#             processed_links.append(link)\n",
    "#             processed_links.extend(process_links(link, cache, visited))\n",
    "#     return processed_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_base = 'https://services.slip.wa.gov.au/public/rest/services/SLIP_Public_Services/Boundaries/MapServer'\n",
    "\n",
    "#retrieve all links\n",
    "cache = {}  # Dictionary to cache retrieved links\n",
    "visited = set()  # Set to track visited URLs\n",
    "\n",
    "all_links = process_links (url_base, cache, visited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "canning_pj1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
